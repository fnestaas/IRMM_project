{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Adversarial Examples\n",
    "This notebook should \n",
    "1. Train new models with different parameters\n",
    "1. for each model, generate both a **targeted** and **untargeted** PGD attack\n",
    "1. Plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path \n",
    "from matplotlib import pyplot as plt \n",
    "import json \n",
    "import optuna \n",
    "from model_utils import find_best_model, choose_model\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "prefix = 'use_test_data/'\n",
    "adv_dir = 'attacks/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for reporting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stats(dir):\n",
    "    with open(dir) as f:\n",
    "        stats = json.load(f)\n",
    "    return stats \n",
    "\n",
    "def get_stats_regular(which=1, duration=100, model='max', root=prefix, dir=None):\n",
    "    \"\"\" \n",
    "    Load stats of model specified directly using the directory of the model, or using which, duration and model name ('max' for latest)\n",
    "    \"\"\"\n",
    "    if dir is None:\n",
    "        dir = Path(root) / f'{which=}_{duration=}'\n",
    "    model = choose_model(dir, model)\n",
    "    dir = Path(model)\n",
    "    return load_stats(dir / 'stats.json') \n",
    "\n",
    "def get_stats_attack(which=1, duration=100, model='max', root=prefix, adv_dir=adv_dir, dir=None, attacks=['targeted', 'targeted_increase', 'targeted_decrease', 'untargeted']):\n",
    "    \"\"\" \n",
    "    Load adversarial stats of model specified directly using the directory of the model, \n",
    "    or using which, duration and model name ('max' for latest).\n",
    "    Returns a dict of dicts, where the outer dict has attack types as keys and the inner uses perturbation sizes\n",
    "    \"\"\"\n",
    "    if dir is None:\n",
    "        # dir is not specified so we find it based on other information\n",
    "        dir = Path(root) / f'{which=}_{duration=}'\n",
    "        model = choose_model(dir, model)\n",
    "        dir = model / adv_dir \n",
    "    # create a dict of dicts based on attack type and perturbation size\n",
    "    retval = {}\n",
    "    for p in dir.iterdir():\n",
    "        key = str(p).split('/')[-1]\n",
    "        perturbation_size = key.split('=')[-1]\n",
    "        for attack in attacks:\n",
    "            if f\"attack_type='{attack}'\" in key: \n",
    "                if not attack in retval.keys():\n",
    "                    retval[attack] = {}\n",
    "                try:\n",
    "                    retval[attack][perturbation_size] = load_stats(p / 'adv_stats.json')\n",
    "                except FileNotFoundError:\n",
    "                    retval[attack][perturbation_size] = {'acc': np.nan, 'cm': np.zeros((3, 3))}\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Models\n",
    "Each with a specific set of parameters, including data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_args(param):\n",
    "    \"\"\"\n",
    "    turn parameter dictionary into list of arguments\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for k, v in param.items():\n",
    "        out.append('--' + k)\n",
    "        out.append(str(v))\n",
    "    return out \n",
    "\n",
    "durations = [10] # [50, 100, 150]\n",
    "whichs = [1]\n",
    "n_classes = 5\n",
    "n_epochs = 30\n",
    "hs = 64 \n",
    "dropout = 1/3\n",
    "num_layers = 2\n",
    "thresholds = 2 # move failure classes closer together\n",
    "opt_time_seconds = 1 # 15*60\n",
    "model_params = {\n",
    "    f'{prefix}{which=}_{duration=}': {\n",
    "        'duration': duration, 'which': which, 'n_classes': n_classes, 'n_epochs': n_epochs,\n",
    "        'dropout': dropout, 'hs': hs, 'num_layers': num_layers, 'thresholds': thresholds\n",
    "    } \n",
    "    for duration in durations for which in whichs\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "for name, v in model_params.items():\n",
    "    which = v['which']\n",
    "    duration = v['duration']\n",
    "    def objective(trial):\n",
    "        v['dropout'] = trial.suggest_float('dropout', 0, .5)\n",
    "        v['num_layers'] = trial.suggest_int('num_layers', 1, 4)\n",
    "        v['hs'] = trial.suggest_int('hs', 64, 256, log=True)\n",
    "        print('\\n\\n', v, '\\n\\n')\n",
    "        subprocess.run(['python', 'lstm.py'] + make_args(v) + ['--target_directory', name]) \n",
    "        acc = get_stats_regular(which=which, duration=duration)['acc']\n",
    "        return -acc # maximize acc\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.enqueue_trial({'dropout': dropout, 'num_layers': num_layers, 'hs': hs})\n",
    "    study.optimize(objective, timeout=opt_time_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = choose_model(Path(f'{prefix}/which=3_{duration=}/'), 'max')\n",
    "# print(model)\n",
    "# subprocess.run(['python', 'test_model.py', '--model_path', model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the best accuracy in the directories\n",
    "# best_accs = {}\n",
    "# for name, v in model_params.items():\n",
    "#     best_accs[name] = find_best_model(name)\n",
    "# print(*[v[1] for v in best_accs.values()], sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples\n",
    "Save the results with the model in the appropriate folder, both for targeted and untargeted attacks. \n",
    "\n",
    "Parameters of the attack:\n",
    "- Attack type (targeted or untargeted)\n",
    "- Perturbation size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues:\n",
    "- There is a bug somewhere because the perturbation_sizes are not respected according to the adv_stats.json\n",
    "- We should implement a keyword for fooling models towards saying models are in better or worse conditions than they really are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the case of breaking the process, it is nice to know the order of the scripts executed\n",
    "# Generate that order\n",
    "order = []\n",
    "attack_types = ['untargeted'] # ['targeted_increase', 'targeted_decrease', 'targeted', 'untargeted']\n",
    "perturbation_sizes = [.1*(i+1) for i in range(5)]\n",
    "for name in model_params.keys():\n",
    "    attack_params = {\n",
    "        f'{adv_dir}{attack_type=}_{perturbation_size=}/': {'attack_type': attack_type, 'perturbation_size': perturbation_size} \n",
    "        for attack_type in attack_types for perturbation_size in perturbation_sizes\n",
    "    }\n",
    "    order.extend([(name, attack_name) for attack_name in attack_params.keys()])\n",
    "\n",
    "def later_experiment(comp, ref, order=order):\n",
    "    \"\"\"\n",
    "    Check if comp is later than ref in order\n",
    "    \"\"\"\n",
    "    if ref == None:\n",
    "        return True\n",
    "    if ref =='skip_all':\n",
    "        return False \n",
    "    j = len(order)\n",
    "    for i, o in enumerate(order):\n",
    "        if str(o[0]) == str(comp[0]) and str(o[1]) == str(comp[1]):\n",
    "            j = i \n",
    "            break \n",
    "    for o in order[:j+1]:\n",
    "        if str(o[0]) == str(ref[0]) and str(o[1]) == str(ref[1]):\n",
    "            return True \n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, make a number of attacks \n",
    "# To that end, make a subfolder \"attacks\" for each of the models\n",
    "\n",
    "# ref = (f'{prefix}/which=4_duration=150', \"attacks/attack_type='targeted'_perturbation_size=0.01/\") \n",
    "ref = None\n",
    "# ref = 'skip_all'\n",
    "\n",
    "for name in model_params.keys():\n",
    "    attack_params = {\n",
    "        f'{adv_dir}{attack_type=}_{perturbation_size=}/': {'attack_type': attack_type, 'perturbation_size': perturbation_size} \n",
    "        for attack_type in attack_types for perturbation_size in perturbation_sizes\n",
    "    }\n",
    "    for attack_name, v in attack_params.items():\n",
    "        if later_experiment((name, attack_name), ref):\n",
    "            print('doing', (name, attack_name))\n",
    "            if 'increase' in name:\n",
    "                preference = 'increase'\n",
    "            elif 'decrease' in name:\n",
    "                preference = 'decrease'\n",
    "            else:\n",
    "                preference = 'None'\n",
    "            subprocess.run(['python', 'attack.py'] + make_args(v) + ['--source_directory', name, '--target_directory', attack_name, '--model_name', 'best', '--preference', preference, '--validation_data', 'False'])\n",
    "        else:\n",
    "            print('skipped', (name, attack_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_of_matrix(cm, critical_class, c_over=15.1, c_under=40):\n",
    "    \"\"\" \n",
    "    Compute the cost according to the relevant confusion matrix\n",
    "    \"\"\"\n",
    "    cm = np.array(cm)\n",
    "    # find the number of cases where we predict a non-critical class when it is in fact critical\n",
    "    n_underestimate = np.sum(cm[critical_class+1:, :critical_class]) # predicted class is critical_class + 1 or higher\n",
    "    # find the number of non-critical cases which are predicted as critical\n",
    "    n_overestimate = np.sum(cm[:critical_class, critical_class+1:])\n",
    "    return (n_underestimate * c_under + n_overestimate * c_over) / np.sum(cm)\n",
    "\n",
    "\n",
    "def plot_statistics(stat='acc', critical_class=3, attacks=['targeted', 'untargeted', 'targeted_increase', 'targeted_decrease']):\n",
    "    def plot_accs(which, attack, filename=None):\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.set_title(f'{attack}_{which=}')\n",
    "        for duration in durations:\n",
    "            key = f'{which=}_{duration=}'\n",
    "            if stat == 'acc':\n",
    "                y = [regular_accs[key]['acc']] + [adv_accs[key][attack][str(s)]['acc'] for s in perturbation_sizes]\n",
    "            elif stat == 'cost':\n",
    "                y = [cost_of_matrix(regular_accs[key]['confusion_matrix'], critical_class)] + [\n",
    "                    cost_of_matrix(adv_accs[key][attack][str(s)]['cm'], critical_class) for s in perturbation_sizes\n",
    "                ]\n",
    "            ax.plot(x, y, label=key)\n",
    "            # ax.set_ylim(.5, 1)\n",
    "            ax.grid()\n",
    "            ax.set_xlabel('perturbation size')\n",
    "            ax.set_ylabel(stat)\n",
    "        ax.legend()\n",
    "        if filename is not None: \n",
    "            dir = Path('/'.join(filename.split('/')[:-1]))\n",
    "            dir.mkdir(exist_ok=True, parents=True)\n",
    "            fig.savefig(filename)\n",
    "        return fig, ax \n",
    "    # get accs\n",
    "    regular_accs = {}\n",
    "    adv_accs = {}\n",
    "    for which in whichs:\n",
    "        for duration in durations:\n",
    "            key = f'{which=}_{duration=}'\n",
    "            regular_accs[key] = get_stats_regular(which, duration, model='best')\n",
    "            adv_accs[key] = get_stats_attack(which, duration, model='best')\n",
    "\n",
    "    # plot targeted results, resulting in a plot per \"which\"\n",
    "    # x=perturbation_size, y=acc\n",
    "    # where we have one line for each duration\n",
    "    x = [0.] + perturbation_sizes \n",
    "    for which in whichs:\n",
    "        for attack in attacks:\n",
    "            fig, ax = plot_accs(which, attack, filename=f'plots/{prefix}/{which=}_{duration=}_{attack=}_{stat=}.png')\n",
    "\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics(stat='acc', attacks=attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics(stat='cost', critical_class=2, attacks=attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out where we could introduce errors\n",
    "attack = 'untargeted'\n",
    "perturbation_size = perturbation_sizes[-1]\n",
    "for which in whichs:\n",
    "    for duration in durations:\n",
    "        chosen_model = choose_model(f'{prefix}/{which=}_{duration=}', 'best')\n",
    "        examples = chosen_model / f'{adv_dir}/attack_type=\\'{attack}\\'_{perturbation_size=}/examples/'\n",
    "        attack_ids = torch.load(examples / 'ids.pt')\n",
    "        engines_times = torch.load(examples/'ids_times.pt')\n",
    "        adv_lbl = torch.load(examples / 'adv_lbl.pt')\n",
    "        print(engines_times) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = choose_model(f'{prefix}/{which=}_{duration=}', 'best')\n",
    "with open(chosen_model /'preds_ys.json') as f:\n",
    "    preds_ys = json.load(f)\n",
    "\n",
    "prev_engine = -1\n",
    "for i, o in enumerate(engines_times):\n",
    "    engine, time = o \n",
    "    if engine != prev_engine: plt.show()\n",
    "    preds = preds_ys['preds'][engine]\n",
    "    ys = preds_ys['ys'][engine]\n",
    "    plt.plot(np.arange(len(preds)), ys, 'o')\n",
    "    plt.plot(np.arange(len(preds)), preds, 'x')\n",
    "    plt.plot(time, adv_lbl[i], '*')\n",
    "    plt.ylim((-.5, 4.5))\n",
    "    prev_engine = engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

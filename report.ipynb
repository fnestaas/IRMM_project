{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Adversarial Examples\n",
    "This notebook should \n",
    "1. Train new models with different parameters\n",
    "1. for each model, generate both a **targeted** and **untargeted** PGD attack\n",
    "1. Plot the results\n",
    "\n",
    "# Problem: data pre-processing messing with which=2, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path \n",
    "from matplotlib import pyplot as plt \n",
    "import json \n",
    "import optuna \n",
    "from model_utils import find_best_model, choose_model\n",
    "\n",
    "prefix = 'three_classes_2/'\n",
    "adv_dir = 'attacks/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for reporting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stats(dir):\n",
    "    with open(dir) as f:\n",
    "        stats = json.load(f)\n",
    "    return stats \n",
    "\n",
    "def get_stats_regular(which=1, duration=100, model='max', root=prefix, dir=None):\n",
    "    \"\"\" \n",
    "    Load stats of model specified directly using the directory of the model, or using which, duration and model name ('max' for latest)\n",
    "    \"\"\"\n",
    "    if dir is None:\n",
    "        dir = Path(root) / f'{which=}_{duration=}'\n",
    "    model = choose_model(dir, model)\n",
    "    dir = Path(model)\n",
    "    return load_stats(dir / 'stats.json') \n",
    "\n",
    "def get_stats_attack(which=1, duration=100, model='max', root=prefix, adv_dir=adv_dir, dir=None):\n",
    "    \"\"\" \n",
    "    Load adversarial stats of model specified directly using the directory of the model, \n",
    "    or using which, duration and model name ('max' for latest).\n",
    "    Returns a dict of dicts, where the outer dict has attack types as keys and the inner uses perturbation sizes\n",
    "    \"\"\"\n",
    "    if dir is None:\n",
    "        # dir is not specified so we find it based on other information\n",
    "        dir = Path(root) / f'{which=}_{duration=}'\n",
    "        model = choose_model(dir, model)\n",
    "        dir = model / adv_dir \n",
    "    # create a dict of dicts based on attack type and perturbation size\n",
    "    retval = {}\n",
    "    for p in dir.iterdir():\n",
    "        key = str(p).split('/')[-1]\n",
    "        perturbation_size = key.split('=')[-1]\n",
    "        if \"attack_type='targeted'\" in key:\n",
    "            if not 'targeted' in retval.keys():\n",
    "                retval['targeted'] = {}\n",
    "            retval['targeted'][perturbation_size] = load_stats(p / 'adv_stats.json')\n",
    "        elif \"attack_type='untargeted'\" in key:\n",
    "            if not 'untargeted' in retval.keys():\n",
    "                retval['untargeted'] = {}\n",
    "            retval['untargeted'][perturbation_size] = load_stats(p / 'adv_stats.json')\n",
    "        elif \"attack_type='targeted_increase'\" in key:\n",
    "            if not 'targeted_increase' in retval.keys():\n",
    "                retval['targeted_increase'] = {}\n",
    "            retval['targeted_increase'][perturbation_size] = load_stats(p / 'adv_stats.json')\n",
    "        elif \"attack_type='targeted_decrease'\" in key:\n",
    "            if not 'targeted_decrease' in retval.keys():\n",
    "                retval['targeted_decrease'] = {}\n",
    "            retval['targeted_decrease'][perturbation_size] = load_stats(p / 'adv_stats.json')\n",
    "        else:\n",
    "            raise NotImplementedError(f'stats not implemented for key {key}')\n",
    "    return retval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Models\n",
    "Each with a specific set of parameters, including data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_args(param):\n",
    "    \"\"\"\n",
    "    turn parameter dictionary into list of arguments\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for k, v in param.items():\n",
    "        out.append('--' + k)\n",
    "        out.append(str(v))\n",
    "    return out \n",
    "\n",
    "durations = [50, 100, 150]\n",
    "whichs = [1, 3]\n",
    "n_classes = 3\n",
    "n_epochs = 30\n",
    "hs = 64 \n",
    "dropout = 1/3\n",
    "num_layers = 2\n",
    "opt_time_seconds = 15*60\n",
    "model_params = {\n",
    "    f'{prefix}{which=}_{duration=}': {\n",
    "        'duration': duration, 'which': which, 'n_classes': n_classes, 'n_epochs': n_epochs,\n",
    "        'dropout': dropout, 'hs': hs, 'num_layers': num_layers, \n",
    "    } \n",
    "    for duration in durations for which in whichs\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "for name, v in model_params.items():\n",
    "    which = v['which']\n",
    "    duration = v['duration']\n",
    "    def objective(trial):\n",
    "        v['dropout'] = trial.suggest_float('dropout', 0, .5)\n",
    "        v['num_layers'] = trial.suggest_int('num_layers', 1, 4)\n",
    "        v['hs'] = trial.suggest_int('hs', 64, 256, log=True)\n",
    "        print('\\n\\n', v, '\\n\\n')\n",
    "        subprocess.run(['python', 'lstm.py'] + make_args(v) + ['--target_directory', name]) \n",
    "        acc = get_stats_regular(which=which, duration=duration)['acc']\n",
    "        return -acc # maximize acc\n",
    "\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(objective, timeout=opt_time_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best accuracy in the directories\n",
    "best_accs = {}\n",
    "for name, v in model_params.items():\n",
    "    best_accs[name] = find_best_model(name)\n",
    "print(*[v[1] for v in best_accs.values()], sep='\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarial Examples\n",
    "Save the results with the model in the appropriate folder, both for targeted and untargeted attacks. \n",
    "\n",
    "Parameters of the attack:\n",
    "- Attack type (targeted or untargeted)\n",
    "- Perturbation size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues:\n",
    "- There is a bug somewhere because the perturbation_sizes are not respected according to the adv_stats.json\n",
    "- We should implement a keyword for fooling models towards saying models are in better or worse conditions than they really are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the case of breaking the process, it is nice to know the order of the scripts executed\n",
    "# Generate that order\n",
    "order = []\n",
    "for name in model_params.keys():\n",
    "    attack_types = ['targeted', 'untargeted']\n",
    "    perturbation_sizes = [.05, .1, .5]# [.01, .05, .1]\n",
    "\n",
    "    attack_params = {\n",
    "        f'{adv_dir}{attack_type=}_{perturbation_size=}/': {'attack_type': attack_type, 'perturbation_size': perturbation_size} \n",
    "        for attack_type in attack_types for perturbation_size in perturbation_sizes\n",
    "    }\n",
    "    order.extend([(name, attack_name) for attack_name in attack_params.keys()])\n",
    "\n",
    "def later_experiment(comp, ref, order=order):\n",
    "    \"\"\"\n",
    "    Check if comp is later than ref in order\n",
    "    \"\"\"\n",
    "    if ref == None:\n",
    "        return True\n",
    "    j = len(order)\n",
    "    for i, o in enumerate(order):\n",
    "        if str(o[0]) == str(comp[0]) and str(o[1]) == str(comp[1]):\n",
    "            j = i \n",
    "            break \n",
    "    for o in order[:j+1]:\n",
    "        if str(o[0]) == str(ref[0]) and str(o[1]) == str(ref[1]):\n",
    "            return True \n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, make a number of attacks \n",
    "# To that end, make a subfolder \"attacks\" for each of the models\n",
    "\n",
    "# ref = (f'{prefix}/which=4_duration=150', \"attacks/attack_type='targeted'_perturbation_size=0.01/\") \n",
    "ref = None\n",
    "\n",
    "for name in model_params.keys():\n",
    "    attack_types = ['targeted_increase', 'targeted_decrease', 'targeted', 'untargeted']\n",
    "    perturbation_sizes = [.1*(i+1) for i in range(5)] \n",
    "\n",
    "    attack_params = {\n",
    "        f'{adv_dir}{attack_type=}_{perturbation_size=}/': {'attack_type': attack_type, 'perturbation_size': perturbation_size} \n",
    "        for attack_type in attack_types for perturbation_size in perturbation_sizes\n",
    "    }\n",
    "    for attack_name, v in attack_params.items():\n",
    "        if later_experiment((name, attack_name), ref):\n",
    "            print('doing', (name, attack_name))\n",
    "            if 'increase' in name:\n",
    "                preference = 'increase'\n",
    "            elif 'decrease' in name:\n",
    "                preference = 'decrease'\n",
    "            else:\n",
    "                preference = 'None'\n",
    "            subprocess.run(['python', 'attack.py'] + make_args(v) + ['--source_directory', name, '--target_directory', attack_name, '--model_name', 'best', '--preference', preference])\n",
    "        else:\n",
    "            print('skipped', (name, attack_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accs\n",
    "regular_accs = {}\n",
    "adv_accs = {}\n",
    "for which in whichs:\n",
    "    for duration in durations:\n",
    "        key = f'{which=}_{duration=}'\n",
    "        regular_accs[key] = get_stats_regular(which, duration, model='best')\n",
    "        adv_accs[key] = get_stats_attack(which, duration, model='best')\n",
    "\n",
    "# plot targeted results, resulting in a plot per \"which\"\n",
    "# x=perturbation_size, y=acc\n",
    "# where we have one line for each duration\n",
    "x = [0.] + perturbation_sizes \n",
    "for which in whichs:\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(f'targeted_{which=}')\n",
    "    for duration in durations:\n",
    "        key = f'{which=}_{duration=}'\n",
    "        y = [regular_accs[key]['acc']] + [adv_accs[key]['targeted'][str(s)]['acc'] for s in perturbation_sizes]\n",
    "        ax.plot(x, y, label=key)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Do the same for the untargeted results\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(f'untargeted_{which=}')\n",
    "    for duration in durations:\n",
    "        key = f'{which=}_{duration=}'\n",
    "        y = [regular_accs[key]['acc']] + [adv_accs[key]['untargeted'][str(s)]['acc'] for s in perturbation_sizes]\n",
    "        ax.plot(x, y, label=key)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Do the same for the untargeted results\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(f'incr_{which=}')\n",
    "    for duration in durations:\n",
    "        key = f'{which=}_{duration=}'\n",
    "        y = [regular_accs[key]['acc']] + [adv_accs[key]['targeted_increase'][str(s)]['acc'] for s in perturbation_sizes]\n",
    "        ax.plot(x, y, label=key)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Do the same for the untargeted results\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(f'decr_{which=}')\n",
    "    for duration in durations:\n",
    "        key = f'{which=}_{duration=}'\n",
    "        y = [regular_accs[key]['acc']] + [adv_accs[key]['targeted_decrease'][str(s)]['acc'] for s in perturbation_sizes]\n",
    "        ax.plot(x, y, label=key)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
